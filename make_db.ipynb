{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"172H6CjuCMTToqbEuwdarSAE4xfBnams-","timestamp":1663801653606},{"file_id":"1qr5eMdjlUBfb51bJu8DeeQVC2HEuAzRm","timestamp":1658151595445}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from datetime import timedelta\n","from PIL import Image\n","import cv2\n","import numpy as np\n","import os\n","import shutil"],"metadata":{"id":"p9UkIeKqYP6V","executionInfo":{"status":"ok","timestamp":1667751909932,"user_tz":180,"elapsed":949,"user":{"displayName":"Guilherme Rodrigues Monteiro","userId":"07250483182311097967"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1fli_EvKQSXD","executionInfo":{"status":"ok","timestamp":1667751935204,"user_tz":180,"elapsed":21825,"user":{"displayName":"Guilherme Rodrigues Monteiro","userId":"07250483182311097967"}},"outputId":"a150717c-4d71-4106-e16f-845fae5d4b4c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["!pip install mediapipe"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jio2Q344N3K7","executionInfo":{"status":"ok","timestamp":1667752008385,"user_tz":180,"elapsed":7568,"user":{"displayName":"Guilherme Rodrigues Monteiro","userId":"07250483182311097967"}},"outputId":"b5cab047-631c-4570-e135-0510474f1ca2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mediapipe\n","  Downloading mediapipe-0.8.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.5 MB)\n","\u001b[K     |████████████████████████████████| 31.5 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.3.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (22.1.0)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.6.0.66)\n","Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.11->mediapipe) (1.15.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.1.1)\n","Installing collected packages: mediapipe\n","Successfully installed mediapipe-0.8.11\n"]}]},{"cell_type":"code","source":["import mediapipe as mp\n","from mediapipe.python.solutions.drawing_utils import _normalized_to_pixel_coordinates\n","mp_face_detection = mp.solutions.face_detection\n","mp_drawing = mp.solutions.drawing_utils"],"metadata":{"id":"8BvwJ2wKYkia","executionInfo":{"status":"ok","timestamp":1667752349561,"user_tz":180,"elapsed":286,"user":{"displayName":"Guilherme Rodrigues Monteiro","userId":"07250483182311097967"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!fusermount -u drive\n","!google-drive-ocamlfuse drive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCcCqizI_ggX","executionInfo":{"status":"ok","timestamp":1667752351047,"user_tz":180,"elapsed":362,"user":{"displayName":"Guilherme Rodrigues Monteiro","userId":"07250483182311097967"}},"outputId":"75d7f3f7-2d8f-4188-8a43-0ede6ed167c6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["fusermount: failed to unmount /content/drive: No such file or directory\n","/bin/bash: google-drive-ocamlfuse: command not found\n"]}]},{"cell_type":"code","source":["%cd /content/gdrive/Shareddrives/TCC_MLR-Guilherme_Vinicius/Notebooks/Files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZ6UoKLWUqN4","executionInfo":{"status":"ok","timestamp":1667752440387,"user_tz":180,"elapsed":1101,"user":{"displayName":"Guilherme Rodrigues Monteiro","userId":"07250483182311097967"}},"outputId":"39cd0887-0068-4484-a294-7736b09d6d8c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/Shareddrives/TCC_MLR-Guilherme_Vinicius/Notebooks/Files\n"]}]},{"cell_type":"code","source":["delete_old_frames()"],"metadata":{"id":"-Nb8XVLiL-Y4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def delete_old_frames(validation=False):\n","  folders = ['./Alto','./Medio','./Baixo']\n","  if validation:\n","    folders = ['./Validation']\n","  for folder in folders:\n","    if os.path.isdir(folder):\n","      shutil.rmtree(folder)"],"metadata":{"id":"gx0wweAqSXpM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def crop_rate(img, x,y,largura,altura, largura_lfw = 96, altura_lfw = 96, interpolation=cv2.INTER_CUBIC): #cv2.INTER_AREA\n","      razao_aspecto = altura_lfw/largura_lfw\n","      centro_x = x + largura/2\n","      centro_y = y + altura/2\n","      area = largura*altura\n","      largura_adj = np.sqrt(area/razao_aspecto)\n","      altura_adj = razao_aspecto*largura_adj\n","      x_min = int(np.floor(centro_x-largura_adj/2))\n","      x_max = int(np.ceil(centro_x+largura_adj/2))\n","      y_min = int(np.floor(centro_y-altura_adj/2 + 0.5))\n","      y_max = int(np.ceil(centro_y+altura_adj/2 + 0.5)) \n","      if y_min <0:\n","         y_max -= y_min\n","         y_min = 0\n","      if x_min <0:\n","         x_max -= x_min\n","         x_min = 0  \n","      # Centralize and crop\n","      crop_img = img[y_min:y_max, x_min:x_max]\n","      img_lfw = cv2.resize(crop_img, (largura_lfw, altura_lfw), interpolation=interpolation)\n","      #print(img_lfw.shape)\n","      return img_lfw\n","\n","def find_face(image):\n","  with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n","    # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n","    results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","\n","    if not results.detections:\n","      return False\n","    image_rows, image_cols, _ = image.shape\n","    max_height = 0\n","    for detection in results.detections:\n","      try:\n","        location_data = detection.location_data\n","        relative_bounding_box = location_data.relative_bounding_box\n","        rect_start_point = _normalized_to_pixel_coordinates(relative_bounding_box.xmin, relative_bounding_box.ymin, image_cols,image_rows)\n","        rect_end_point = _normalized_to_pixel_coordinates(relative_bounding_box.xmin+relative_bounding_box.width, relative_bounding_box.ymin+relative_bounding_box.height, image_cols, image_rows)\n","        xleft,ytop = rect_start_point\n","        xright,ybot = rect_end_point\n","        width = xright-xleft\n","        height = ybot-ytop\n","        if height > max_height:\n","          max_height = height\n","          bounding_box = [\n","              xleft, ytop,\n","              width, height,\n","          ]\n","\n","        return bounding_box\n","      except:\n","        return False"],"metadata":{"id":"1YBF7OrGY8Ar","executionInfo":{"status":"ok","timestamp":1667752499269,"user_tz":180,"elapsed":265,"user":{"displayName":"Guilherme Rodrigues Monteiro","userId":"07250483182311097967"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["SAVING_FRAMES_PER_SECOND = 1 # numero de frames a serem salvos por segundo\n","\n","def format_timedelta(td):\n","    \"\"\"Utility function to format timedelta objects in a cool way (e.g 00:00:20.05) \n","    omitting microseconds and retaining milliseconds\"\"\"\n","    result = str(td)\n","    try:\n","        result, ms = result.split(\".\")\n","    except ValueError:\n","        return result + \".00\".replace(\":\", \"-\")\n","    ms = int(ms)\n","    ms = round(ms / 1e4)\n","    return f\"{result}.{ms:02}\".replace(\":\", \"-\")\n","\n","def get_saving_frames_durations(cap, saving_fps):\n","    \"\"\"A function that returns the list of durations where to save the frames\"\"\"\n","    s = []\n","    # get the clip duration by dividing number of frames by the number of frames per second\n","    clip_duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS)\n","    # use np.arange() to make floating-point steps\n","    for i in np.arange(0, clip_duration, 1 / saving_fps):\n","        s.append(i)\n","    return s\n","\n","def make_frames(video_file, validation=False):\n","    filename, _ = os.path.splitext(video_file)\n","    # make a folder by the name of the video file\n","    if not os.path.isdir(filename):\n","        os.mkdir(filename)\n","    # read the video file    \n","    cap = cv2.VideoCapture(video_file)\n","    # get the FPS of the video\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    # if the SAVING_FRAMES_PER_SECOND is above video FPS, then set it to FPS (as maximum)\n","    saving_frames_per_second = min(fps, SAVING_FRAMES_PER_SECOND)\n","    # get the list of duration spots to save\n","    saving_frames_durations = get_saving_frames_durations(cap, saving_frames_per_second)\n","    # start the loop\n","    count = 0\n","    while True:\n","        is_read, frame = cap.read()\n","        if not is_read:\n","            # break out of the loop if there are no frames to read\n","            break\n","        # get the duration by dividing the frame count by the FPS\n","        frame_duration = count / fps\n","        try:\n","            # get the earliest duration to save\n","            closest_duration = saving_frames_durations[0]\n","        except IndexError:\n","            # the list is empty, all duration frames were saved\n","            break\n","        if frame_duration >= closest_duration:\n","            # if closest duration is less than or equals the frame duration, \n","            # then save the frame\n","            frame_duration_formatted = format_timedelta(timedelta(seconds=frame_duration))\n","            pathname = os.path.join(filename, f\"frame{frame_duration_formatted}.jpg\")\n","            cv2.imwrite(pathname, frame)\n","            img = cv2.imread(pathname)\n","            face = find_face(img)\n","            if face:\n","              [x, y,largura,altura] = face\n","              gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n","              #print(img.size)\n","              person = crop_rate(gray,x=x,y=y,largura=largura,altura=altura)\n","              #print(img2.size)\n","              #break\n","              cv2.imwrite(pathname, person)\n","              # drop the duration spot from the list, since this duration spot is already saved\n","              try:\n","                  saving_frames_durations.pop(0)\n","              except IndexError:\n","                  pass\n","            else:\n","              os.remove(pathname)\n","        # increment the frame count\n","        count += 1"],"metadata":{"id":"2VTtv5kVT5fi","executionInfo":{"status":"ok","timestamp":1667756602209,"user_tz":180,"elapsed":262,"user":{"displayName":"Guilherme Rodrigues Monteiro","userId":"07250483182311097967"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ck4vde7rBqAE","executionInfo":{"status":"ok","timestamp":1667754762057,"user_tz":180,"elapsed":312,"user":{"displayName":"Guilherme Rodrigues Monteiro","userId":"07250483182311097967"}},"outputId":"1b6896d7-ccb7-4ab2-e0d0-77728fc2d46e"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["model_clf_SVM_Gaussiano.sav  Pain_Level_Images_RA2  Videos\n","P8_Baixo.mp4\t\t     pca.pkl\t\t    Videos_Archives\n","Pain_Level_Images\t     Validation\t\t    Videos_Extract\n","Pain_Level_Images_Backup     Video\t\t    Voluntario_10_2.mp4\n"]}]},{"cell_type":"code","source":["make_frames(\"./Videos\")"],"metadata":{"id":"CMkMED2MZSlN","executionInfo":{"status":"error","timestamp":1667756736045,"user_tz":180,"elapsed":130924,"user":{"displayName":"Guilherme Rodrigues Monteiro","userId":"07250483182311097967"}},"colab":{"base_uri":"https://localhost:8080/","height":380},"outputId":"e2f3d4a2-3027-479d-c74a-cc422ca44d41"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["./Videos_Extract/Voluntario1_1.mp4\n","./Videos_Extract/Voluntario1_1\n"]},{"output_type":"error","ename":"ZeroDivisionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-0e65eaccb19e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./Videos_Extract\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mmake_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-0fd179928239>\u001b[0m in \u001b[0;36mmake_frames\u001b[0;34m(video_file, validation)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msaving_frames_per_second\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVING_FRAMES_PER_SECOND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# get the list of duration spots to save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msaving_frames_durations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_saving_frames_durations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_frames_per_second\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;31m# start the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-0fd179928239>\u001b[0m in \u001b[0;36mget_saving_frames_durations\u001b[0;34m(cap, saving_fps)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# get the clip duration by dividing number of frames by the number of frames per second\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mclip_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FRAME_COUNT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# use np.arange() to make floating-point steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msaving_fps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"]}]}]}